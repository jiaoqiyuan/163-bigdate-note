# HA 原理

## Hadoop2.0 产生背景

- Hadoop1.0 中 `HDFS` 和 `MapReduce` 在高可用、扩展性方面存在问题。

- `HDFS` 存在的问题：

    - Namenode单点故障问题 ---> HA

    - NameNode压力过大问题，且内存受限，影响扩展 ---> YARN

- `MapReduce` 存在的问题：

    - JobTracker 访问压力大，影响系统扩展性

    - 难以支持除 MapReduce 之外的计算框架，如 Spark、Storm等。

## Hadoop2.x 和 Hadoop1.x

![1](https://github.com/jiaoqiyuan/pics/raw/master/sxt/hadoop1.x2.x.png)

可以看到 Hadoop2.x 是在 Hadoop1.x 基础上添加了 YARN，从而支持了更多的计算框架，同时将 namenode 从繁重的任务重解脱出来，不在负责任务调度任务，转而由 YARN 进行任务调度的管理。

## HDFS2.x

HDFS2.x 解决了 HDFS1.x中单点故障和内存受限的问题。

- 单点故障问题通过 HDFS HA 解决，通过主备 NameNode 方式，如果主 NameNode 发生故障，则切换到备 NameNode 上。这里牵扯到有主从 NameNode 元数据同步问题。

- 内存受限的问题：

    - HDFS Federation (联邦)

    - 水平扩展，支持多个NameNode

        - 每个 NameNode分管一部分目录

        - 所有 NameNode 共享所有 DataNode 存储资源

- 2.x 仅是架构上发生了变化，使用方式不变。

## Hadoop HA

![2](https://github.com/jiaoqiyuan/pics/raw/master/sxt/hadoop-ha.png)

上面图中展示的 `HA` 是基于 ZooKeeper 实现的自动切换 NameNode 节点的高可用性系统, 该系统下有两个 `NameNode`, 二者之间的切换是通过 ZooKeeper 操作的。

这里的高可用性实现思路是设置两个 NameNode ，一主一从，正常情况下主 NameNode 负责管理 HDFS 上的元数据信息，当主 NameNode 节点出现故障时，由 Zookeeper 切换到从 NameNode 继续提供服务，此时从 NameNode 就变为了主 NameNode，然后由相关人员修复之前坏掉的 NameNode。这两个 NameNode 需要经常同步信息，保证二者的内容尽可能时一致的，以免主 NameNode 出现问题切换到从 NameNode 时二者信息不一致导致的数据丢失问题。

客户端通过主 NameNode 操作数据，由客户端操作产生的数据信息只会记录到主 NameNode 中，要想主从 NameNode 数据保持一致，就需要一种主从 NameNode 的同步机制。

这里使用的同步机制就是 JournalNodes (日志节点) ，如果采用同步阻塞机制，中间数据传输过程中可能出现网络波动问题，导致阻塞不能返回结果而失败的问题，为了一致性破坏了可用性，所以阻塞的同步机制不能采用。

这里 Hadoop2.x 采用的同步思路是通过多个第三方日志节点(JournalNodes)记录用户对 NameNode 的操作(edits记录)，Linux下有一种 NFS 网络文件系统，可以使用网络文件系统文件记录 NameNode的内容，这样另一个从 NameNode 也能同步拥有相同的数据啦。但是这个系统还是存在单点故障的问题，还是不能说是高可用。

- `ZooKeeper` 是做分布式协调的管理系统，上图的 JVM 进程 `FailoverController` 和 `NameNode` 是成对出现的，负责监视 `NameNode` 的状态。

- 系统启动时由两个 `FailoverController` 通过争抢锁的机制决定主从身份后，设置两个 `NameNode` 分别为 Active 和 Standby。


# HA 搭建

|  | NN-1 | NN-2 | DN | ZK | ZKFC | JNN |
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| node01 | * |   |   |   | * | * |
| node02 |   | * | * | * | * | * |
| node03 |   |   | * | * |   | * |
| node04 |   |   | * | * |   |   |

## 搭建 ZooKeeper

1. 下载 [ZooKeeper3.4.6](https://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz)到 node02 节点，现在 先在node02 节点上配置好，然后再分发到 node03/node04 两个节点。

    解压 ZooKeeper 到 `/opt/sxt` 目录下，然后添加环境变量到 `/etc/profile` 文件中：

    ```conf
    # ZooKeeper
    export ZOOKEEPER_HOME=/opt/sxt/zookeeper-3.4.6
    export PATH=$ZOOKEEPER_HOME/bin:$PATH
    ```

    生效配置文件：

    ```
    [root@node02 hadoop]# source /etc/profile
    ```

    同时在 `node03/node04` 上进行同样的操作。

2. 配置 `conf/zoo.cfg` :

    进入 ZooKeeper 的 `conf` 目录，拷贝一份 `zoo_sample.cfg`：

    ```
    [root@node02 conf]# cp zoo_sample.cfg zoo.cfg
    ```

    配置 `zoo.cfg` ：

    ```conf
    dataDir=/var/sxt/hadoop/zk
    clientPort=2181
    server.1=172.16.123.12:2888:3888
    server.2=172.16.123.13:2888:3888
    server.3=172.16.123.14:2888:3888
    ```

    在 `node02/node03/node04` 上分别创建 `/var/sxt/hadoop/zk/myid` 文件，并分别写入 `1/2/3`：

    ```
    node02:
    [root@node02 hadoop]# mkdir -p /var/sxt/hadoop/zk/
    [root@node02 hadoop]# echo 1 > /var/sxt/hadoop/zk/myid

    node03:
    [root@node03 hadoop]# mkdir -p /var/sxt/hadoop/zk/
    [root@node03 hadoop]# echo 2 > /var/sxt/hadoop/zk/myid

    node04:
    [root@node04 hadoop]# mkdir -p /var/sxt/hadoop/zk/
    [root@node04 hadoop]# echo 3 > /var/sxt/hadoop/zk/myid   
    ```

3. 分发 `node02` 上的 `/opt/sxt/zookeeper-3.4.6` 目录到 `node03/node4`：

    ```
    [root@node02 hadoop]# scp -r /opt/sxt/zookeeper-3.4.6/ root@node03:/opt/sxt
    [root@node02 hadoop]# scp -r /opt/sxt/zookeeper-3.4.6/ root@node04:/opt/sxt
    ```

    确认 `node03/node04` 上的 `/var/sxt/hadoop/zk/myid` 中的数字是否为对应的id号，`node02/node03/node04` 分别对应 `1/2/3`。

    确认 `node03/node04` 上的 `/etc/profile` 文件中已经配置 ZooKeeper 环境变量。

4. 启动 ZooKeeper

    使用脚本先启动 `node02` 上的 zk：

    ```
    [root@node02 hadoop]# zkServer.sh start
    [root@node02 hadoop]# zkServer.sh status
    JMX enabled by default
    Using config: /opt/sxt/zookeeper-3.4.6/bin/../conf/zoo.cfg
    Error contacting service. It is probably not running.

    ```

    然后依次启动 `node03/node04` 上的 zk：

    ```
    [root@node03 hadoop]# zkServer.sh start
    [root@node04 hadoop]# zkServer.sh start
    ```

    最后可以看到 node03 成为了 leader， node02/node04 成为了 follower。

## 搭建Hadoop HA

1. 备份之前的分布式 hadoop 的配置文件 `etc/hadoop` 为 `etc/hadoop-full` ，以备不时之需：

    ```
    [root@node01 etc]# cp -r hadoop/ hadoop-full
    ```

2. 在 node02 上配置到 自身和 node01 的免秘钥：

    ```
    在 node02 上：
    [root@node02 hadoop]# ssh-keygen -t dsa -P '' -f /root/.ssh/id_dsa
    [root@node02 hadoop]# cat id_dsa.pub >> authorized_keys
    [root@node02 hadoop]# scp id_dsa.pub node01:/root/.ssh/node02.pub

    在 node01 上：
    [root@node01 hadoop]# cat node02.pub >> authorized_keys
    ```

2. 修改 `hdfs-site.xml` ,修改为如下内容：

    ```
    <property>
        <name>dfs.nameservices</name>
        <value>mycluster</value>
    </property>

    <property>
        <name>dfs.ha.namenodes.mycluster</name>
        <value>nn1,nn2</value>
    </property>

    <property>
        <name>dfs.namenode.rpc-address.mycluster.nn1</name>
        <value>node01:8020</value>
    </property>

    <property>
        <name>dfs.namenode.rpc-address.mycluster.nn2</name>
        <value>node02:8020</value>
    </property>

    <property>
        <name>dfs.namenode.http-address.mycluster.nn1</name>
        <value>node01:50070</value>
    </property>

    <property>
        <name>dfs.namenode.http-address.mycluster.nn2</name>
        <value>node02:50070</value>
    </property>

    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://node01:8485;node02:8485;node03:8485/mycluster</value>
    </property>

    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/var/sxt/hadoop/ha/jn</value>
    </property>

    <property>
        <name>dfs.client.failover.proxy.mycluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>

    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>

    <property>
        <name>dfs.ha.fencing.ssh.provate-key-files</name>
        <value>/root/.ssh/id_dsa</value>
    </property>

    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    ```

3. 修改 `core-site.xml` 文件内容：

    ```
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://mycluster</value>
    </property>

    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/sxt/hadoop/ha</value>
    </property>

    <property>
        <name>ha.zookeeper.quorum</name>
        <value>node02:2181,node03:2181,node04:2181</value>
    </property>
    ```

4. 启动journal node

    在 `node01/node02/node03` 上启动：

    ```
    [root@node01 hadoop]# hadoop-daemon.sh start journalnode
    [root@node02 hadoop]# hadoop-daemon.sh start journalnode
    [root@node03 hadoop]# hadoop-daemon.sh start journalnode
    ```
    
5. 在 `node01` 上格式化 namenode：

    ```
    [root@node01 hadoop]# hdfs namenode -format
    ```
6. 同步 node01/node02 的 namenode：
    
    启动 node01 上的 namenode：

    在node01上运行：

    ```
    [root@node01 hadoop]# hadoop-daemon.sh start namenode
    ```

    在node02上运行：

    ```
    [root@node02 hadoop]# hdfs namenode -bootstrapStandby
    ```

7. 启动 zkfc 格式化 ZooKeeper：

    在 node01 上运行：
    
    ```
    [root@node01 hadoop]# hdfs zkfc -formatZK
    ```

8. 在 ZooKeeper 节点上启动 zk：

    ```
    [root@node04 ~]# zkCli.sh start
    [zk: localhost:2181(CONNECTED) 0] ls /
    [zookeeper, hadoop-ha]
    [zk: localhost:2181(CONNECTED) 1] ls /hadoop-ha
    [mycluster]
    [zk: localhost:2181(CONNECTED) 2] ls /hadoop-ha/mycluster
    []
    ```

9. 启动dfs：

    ```
    [root@node01 hadoop]# start-dfs.sh 
    ```

    在网页端查看 `node01:50070` 和 `node02:50070` namenode信息。